{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bd4295a-9f5b-4a85-b8c8-c2db9b7f32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def extract_labels(labels):\n",
    "    # Map emotion labels to integers\n",
    "    label_mapping = {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n",
    "    return np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataset_path, transform=None):\n",
    "        self.data, self.labels = self.load_data(dataset_path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_data(dataset_path):\n",
    "        data = []\n",
    "        labels = []\n",
    "\n",
    "        for emotion_folder in os.listdir(dataset_path):\n",
    "            emotion_path = os.path.join(dataset_path, emotion_folder)\n",
    "            for img_name in os.listdir(emotion_path):\n",
    "                img_path = os.path.join(emotion_path, img_name)\n",
    "                img = cv2.imread(img_path)\n",
    "                data.append(img)\n",
    "                labels.append(emotion_folder)\n",
    "\n",
    "        return np.array(data), extract_labels(labels)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4041eb82-fa0c-4e1a-873a-441df08b3433",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97ebc8bd-8ea6-4145-98dd-583275bfebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'FER2013/train'\n",
    "absolute_train_path = os.path.abspath(train_path)\n",
    "\n",
    "test_path = 'FER2013/test'\n",
    "absolute_test_path = os.path.abspath(test_path)\n",
    "\n",
    "train_dataset = EmotionDataset(absolute_train_path, transform=transform)\n",
    "test_dataset = EmotionDataset(absolute_test_path, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "345e652f-295f-45f9-b649-52dbece0983a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for computation!\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # Use the GPU for computation\n",
    "    print(\"Using GPU for computation!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # Use the CPU for computation\n",
    "    print(\"Using CPU for computation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95861a7b-ff09-4e4d-ab4a-64474f34848a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.models' has no attribute 'face_resnet50'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfacenet_pytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InceptionResnetV1 \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m face_model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mface_resnet50(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m face_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20180402-114759-step1196000.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      7\u001b[0m face_model\u001b[38;5;241m.\u001b[39meval() \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchvision.models' has no attribute 'face_resnet50'"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, models\n",
    "from facenet_pytorch import InceptionResnetV1 \n",
    "import torch.nn as nn\n",
    "\n",
    "face_model = models.face_resnet50(pretrained=False)\n",
    "face_model.load_state_dict(torch.load('20180402-114759-step1196000.pth'))\n",
    "face_model.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Freeze early layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "model.last_linear.weight.requires_grad = True\n",
    "model.last_linear.bias.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfad7a7-e4b7-47ea-a1aa-914a7d03aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b62b97-9b78-4d91-b8ba-35eee6a4112a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    train_accuracy = 0\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    for inputs, labels in train_loader: \n",
    "     \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        inputs = inputs.to(device) \n",
    "        labels = labels.long().to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        num_correct += (preds == labels).sum()\n",
    "        num_samples += labels.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        train_accuracy += acc\n",
    "        \n",
    "    # Print epoch loss and accuracy\n",
    "    avg_acc = train_accuracy / len(train_loader)            \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {avg_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee262859-9523-4996-ae3a-b9ab376916b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
